{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "Autoencoder are the neural networks  that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. This kind of network is composed of two parts:\n",
    "\n",
    "* Encoder: This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function h=f(x).\n",
    "\n",
    "* Decoder: This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function r=g(h).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*V_YtxTFUqDrmmu2JqMZ-rA.png)\n",
    "\n",
    "The autoencoder as a whole can thus be described by the function g(f(x)) = r where you want r as close as the original input x.\n",
    "\n",
    "#### Purpose: \n",
    "The only purpose of autoencoders was to copy the input to the output, they would be useless. Indeed, we hope that, by training the autoencoder to copy the input to the output, the latent representation h will take on useful properties.\n",
    "\n",
    "### Practical Usage:\n",
    "Today data denoising and dimensionality reduction for data visualization are considered as two main interesting practical applications of autoencoders. \n",
    "\n",
    "Autoencoders are learned automatically from data examples. It means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input and that it does not require any new engineering, only the appropriate training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of AutoEncoders\n",
    "\n",
    "1. Vanilla autoencoder\n",
    "2. Multilayer autoencoder\n",
    "3. Convolutional autoencoder\n",
    "4. Regularized autoencoder\n",
    "\n",
    "### Vanilla autoencoders\n",
    "In its simplest form, the autoencoder is a three layers net, i.e. a neural net with one hidden layer. The input and output are the same, and we learn how to reconstruct the input, for example using the adam optimizer and the mean squared error loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 64\n",
    "output_size = 784\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "\n",
    "# Encoder\n",
    "h = Dense(hidden_size, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "r = Dense(output_size, activation='sigmoid')(h)\n",
    "\n",
    "autoencoder = Model(input=x, output=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer AutoEncoders\n",
    "\n",
    "If one hidden layer is not enough, we can obviously extend the autoencoder to more hidden layers. Now our implementation uses 3 hidden layers instead of just one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 128\n",
    "code_size = 64\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "\n",
    "# Encoder\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(x)\n",
    "h = Dense(code_size, activation='relu')(hidden_1)\n",
    "\n",
    "# Decoder\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(h)\n",
    "r = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "\n",
    "autoencoder = Model(input=x, output=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Autoencoders\n",
    "\n",
    "Convolutional Autoencoders (CAE),on the other way, use the convolution operator to accommodate this observation. Convolution operator allows filtering an input signal in order to extract some part of its content. They learn to encode the input in a set of simple signals and then try to reconstruct the input from them.\n",
    "\n",
    "![A convolution between a 4x4x1 input and a 3x3x1 convolutional filter. \n",
    "The result is a 2x2x1 activation map.](https://cdn-images-1.medium.com/max/800/1*cFMF_uWgUFdVRMZAZ0Bfzg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28, 28,1)) \n",
    "\n",
    "# Encoder\n",
    "conv1_1 = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "pool1 = MaxPooling2D((2, 2), padding='same')(conv1_1)\n",
    "conv1_2 = Conv2D(8, (3, 3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D((2, 2), padding='same')(conv1_2)\n",
    "conv1_3 = Conv2D(8, (3, 3), activation='relu', padding='same')(pool2)\n",
    "h = MaxPooling2D((2, 2), padding='same')(conv1_3)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "conv2_1 = Conv2D(8, (3, 3), activation='relu', padding='same')(h)\n",
    "up1 = UpSampling2D((2, 2))(conv2_1)\n",
    "conv2_2 = Conv2D(8, (3, 3), activation='relu', padding='same')(up1)\n",
    "up2 = UpSampling2D((2, 2))(conv2_2)\n",
    "conv2_3 = Conv2D(16, (3, 3), activation='relu')(up2)\n",
    "up3 = UpSampling2D((2, 2))(conv2_3)\n",
    "r = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up3)\n",
    "\n",
    "autoencoder = Model(input=x, output=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Autoencoder\n",
    "\n",
    "regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output. In practice, we usually find two types of regularized autoencoder: the sparse autoencoder and the denoising autoencoder.\n",
    "\n",
    "#### Sparse autoencoder : \n",
    "Sparse autoencoders are typically used to learn features for another task such as classification. An autoencoder that has been regularized to be sparse must respond to unique statistical features of the dataset it has been trained on, rather than simply acting as an identity function. In this way, training to perform the copying task with a sparsity penalty can yield a model that has learned useful features as a byproduct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 64\n",
    "output_size = 784\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "\n",
    "# Encoder\n",
    "h = Dense(hidden_size, activation='relu', activity_regularizer=regularizers.l1(10e-5))(x)\n",
    "\n",
    "# Decoder\n",
    "r = Dense(output_size, activation='sigmoid')(h)\n",
    "\n",
    "autoencoder = Model(input=x, output=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in our hidden layer, we added an l1 activity regularizer, that will apply a penalty to the loss function during the optimization phase. As a result, the representation is now sparser compared to the vanilla autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising autoencoder : \n",
    "Autoencoders are Neural Networks which are commonly used for feature selection and extraction. Denoising Autoencoders by corrupting the data on purpose by randomly turning some of the input values to zero. In general, the percentage of input nodes which are being set to zero is about 50%. Other sources suggest a lower count, such as 30%. It depends on the amount of data and input nodes you have.\n",
    "\n",
    "When calculating the Loss function, it is important to compare the output values with the original input, not with the corrupted input. \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*G0V4dz4RKTKGpebeoSWB0A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "conv1_1 = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "pool1 = MaxPooling2D((2, 2), padding='same')(conv1_1)\n",
    "conv1_2 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
    "h = MaxPooling2D((2, 2), padding='same')(conv1_2)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "conv2_1 = Conv2D(32, (3, 3), activation='relu', padding='same')(h)\n",
    "up1 = UpSampling2D((2, 2))(conv2_1)\n",
    "conv2_2 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)\n",
    "up2 = UpSampling2D((2, 2))(conv2_2)\n",
    "r = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)\n",
    "\n",
    "autoencoder = Model(input=x, output=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
